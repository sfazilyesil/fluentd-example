<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

<source>
  @type monitor_agent
  bind 0.0.0.0
  port 24220
</source>

<source>
  @type tail
  <parse>
    @type none
  </parse>
  path /var/logs/*unformatted*.log
  pos_file /var/data/td-agent/track.unformatted.pos
  path_key filename 
  tag file
</source>

<source>
  @type tail
  <parse>
    @type json
  </parse>
  path /var/logs/*json-formatted*.log
  pos_file /var/data/td-agent/track.json-formatted.pos
  path_key filename 
  tag file
</source>

<filter file>
  @type record_transformer
  enable_ruby
  remove_keys n,_dummy_
  <record>
    hostname ${hostname}
    app "file"
    _dummy_ ${if record.has_key?("m") && record["m"].is_a?(Hash) && record["m"].has_key?("m2") && record["m"]["m2"].is_a?(Hash); record["m"]["m2"] = record["m"]["m2"].to_json(); end; nil}
  </record>
  
</filter>

<filter web1>
  @type parser
  key_name "$.log"
  hash_value_field "message"
  reserve_data true 
  <parse>
    @type apache2
    keep_time_key true
  </parse>
  tag web1json
</filter>

<filter web1json>
  @type record_transformer
  <record>
    hostname ${hostname}
    app "web1"
  </record>
</filter>

<filter web2>
  @type record_transformer
  <record>
    hostname ${hostname}
    app "web2"
  </record>
</filter>

<filter nodejs>
  @type concat
  key log
  stream_identity_key container_id
  multiline_start_regexp /^Err:/
  continuous_line_regexp /at\s/
</filter>

<match nodejs>
  @type copy
  <store>
    @type file
    path /var/data/${tag}/%Y%m%d/${tag}-%Y%m%d%H%M
    compress gzip
    <format>
      @type json
      localtime true
    </format>
    <buffer tag,time>
      @type file
      timekey_wait 10s
      timekey 1m
      timekey_use_utc false
      path /var/data/${tag}/docker-logs
    </buffer>
    <inject>
      time_format %Y%m%dT%H%M%S%z
      localtime true
      timekey 1d
    </inject>
  </store>
  <store>
    @type kafka_buffered

    # list of seed brokers
    brokers localhost:9092

    # buffer settings
    buffer_type file
    buffer_path /buffer/td-file-kafka
    flush_interval 3s

    # topic settings
    default_topic log-messages

    # data type settings
    output_data_type json
    compression_codec gzip

    # prod
    ucer settings
    max_send_retries 1
    required_acks -1
  </store>
</match>

<match **>
    @type kafka_buffered

    # list of seed brokers
    brokers localhost:9092

    # buffer settings
    buffer_type file
    buffer_path /buffer/td
    flush_interval 3s

    # topic settings
    default_topic log-messages

    # data type settings
    output_data_type json
    compression_codec gzip

    # prod
    ucer settings
    max_send_retries 1
    required_acks -1
</match>
