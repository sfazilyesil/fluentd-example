<system>
  #suppress_config_dump
</system>

@include metrics.conf

<source>
  @type forward
  port "#{ENV['FORWARD_PORT']}"
  bind 0.0.0.0
</source>

<source>
  @type tail
  <parse>
    @type none
  </parse>
  path /var/logs/*unformatted*.log
  pos_file /var/data/td-agent/track.unformatted.pos
  path_key filename 
  tag file.unformatted
</source>

<source>
  @type tail
  <parse>
    @type json
  </parse>
  path /var/logs/*json-formatted*.log
  pos_file /var/data/td-agent/track.json-formatted.pos
  path_key filename 
  tag file.formatted
</source>

<filter file**>
  @type record_modifier
  remove_keys _dummy_
  <record>
   _dummy_ ${if record["msg"].is_a?String && record["msg"].length > 100; record["msg"]=record["msg"][0,100] << "...Truncated...MessageIsTooLarge..."; end; nil}
  </record>
</filter>

<filter file**>
  @type record_modifier
  remove_keys removeMe,_dummy_
  <record>
    hostname "#{Socket.gethostname}"
    app "fex"
    tag ${tag}
    msgStr ${if record.has_key?("msg"); record["msg"].to_json(); end;}
    _dummy_ ${if record.has_key?("m") && record["m"].is_a?(Hash) && record["m"].has_key?("m2") && record["m"]["m2"].is_a?(Hash); record["m"]["m2"] = record["m"]["m2"].to_json(); end; nil}
  </record>
</filter>

<filter web1>
  @type parser
  key_name "$.log"
  hash_value_field "message"
  reserve_data true 
  <parse>
    @type apache2
    keep_time_key true
  </parse>
  tag web1json
</filter>

<filter web1json>
  @type record_modifier
  <record>
    hostname "#{Socket.gethostname}"
    app "fex"
    tag ${tag}
  </record>
</filter>

<filter web2>
  @type record_modifier
  <record>
    hostname "#{Socket.gethostname}"
    app "fex"
    tag ${tag}
  </record>
</filter>

<match file**>
  @type relabel
  @label @KAFKA_OUTPUT
</match>

<match web*>
  @type relabel
  @label @KAFKA_OUTPUT
</match>

<match nodejs*>
  @type detect_exceptions
  remove_tag_prefix xyz
  message log
  languages js
  stream container_id
  multiline_flush_interval 0.5
  @label @NODEJS_PROCESS
</match>

<label @NODEJS_PROCESS>
  <filter **>
    @type parser
    reserve_data true
    key_name log
    <parse>
      @type grok
      <grok>
        pattern %{TIMESTAMP_ISO8601:timestamp} - %{LOGLEVEL:level} - %{GREEDYDATA:msg}
      </grok>
      <grok>
        pattern \[%{TIMESTAMP_ISO8601:timestamp}\] - %{LOGLEVEL:level} - %{GREEDYDATA:msg}
      </grok>
      <grok>
        pattern %{TIMESTAMP_ISO8601:timestamp} - %{GREEDYDATA:msg}
      </grok>
      <grok>
        pattern \[%{TIMESTAMP_ISO8601:timestamp}\] - %{GREEDYDATA:msg}
      </grok> 
      <grok>
        pattern %{GREEDYDATA:msg}
      </grok>    
    </parse>
  </filter>
  <filter **>
    @type record_modifier
    remove_keys _dummy_, log
    <record>
      tag ${tag}
      app "nodejs"
     _dummy_ ${if record["msg"].length > 500; record["msg"]= record["msg"][0,500] << "...Truncated...MessageIsTooLarge..."; end; nil}
    </record>
  </filter>
  <match **>
    @type route
    <route **>
      copy
      @label @KAFKA_OUTPUT 
    </route>
    <route **>
      copy
      @label @FILE_OUTPUT
    </route>
  </match>
</label>


<label @KAFKA_OUTPUT>
  <match **>
      @type kafka_buffered

      # kafka brokers
      brokers "#{ENV['KAFKA_BROKERS']}"
      use_event_time true

      # buffer settings
      buffer_type file
      buffer_path /var/data/kafka-buffer
      flush_interval 2s

      # topic settings
      default_topic log-messages

      # data type settings
      output_data_type json

      # producer settings
      chunk_limit_size 500k
      max_send_limit_bytes 20000
      max_send_retries 10
      required_acks -1
  </match>
</label>

<label @FILE_OUTPUT>
  <match **>
    @type file
    path /var/data/${tag}/%Y%m%d/${tag}-%Y%m%d-%H
    compress gzip
    append true
    <format>
      @type json
      localtime true
    </format>
    <buffer tag,time>
      @type file
      timekey 1h
      flush_mode interval
      flush_interval 1m
      timekey_use_utc false
      timekey_zone Asia/Riyadh
      path /var/data/${tag}
    </buffer>
  </match>
</label>
